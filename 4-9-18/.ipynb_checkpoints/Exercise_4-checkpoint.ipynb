{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Elias               \n",
      "  Num: Unary       : Binary     : Gamma      : Delta      : Goloumb   \n",
      "    0: 0           : 0          : 0          : 0          : 00        \n",
      "    1: 10          : 1          : 100        : 1000       : 01        \n",
      "    2: 110         : 10         : 1100       : 11000      : 010       \n",
      "    3: 1110        : 11         : 1101       : 11001      : 100       \n",
      "    4: 11110       : 100        : 11100      : 11010      : 101       \n",
      "    5: 111110      : 101        : 11101      : 11011      : 1010      \n",
      "    6: 1111110     : 110        : 111010     : 110110     : 1100      \n",
      "    7: 11111110    : 111        : 111011     : 110111     : 1101      \n",
      "    8: 111111110   : 1000       : 111100     : 111000     : 11010     \n",
      "    9: 1111111110  : 1001       : 111101     : 111001     : 11100     \n",
      "   10: 11111111110 : 1010       : 1111010    : 1110010    : 11101     \n"
     ]
    }
   ],
   "source": [
    "##Implement Elias Delta and Elias Gamma and Golomb coding in python\n",
    "from math import log,ceil\n",
    "\n",
    "log2 = lambda x: log(x,2)\n",
    "\n",
    "def binary(x, l = 1):\n",
    "\tfmt = '{0:0%db}'%1\n",
    "\treturn fmt.format(x)\n",
    "\n",
    "def unary(x):\n",
    "\treturn x*'1'+'0'\n",
    "\n",
    "def elias_generic(lencoding, x):\n",
    "\tif x == 0: return '0'\n",
    "\tl = 1+int(log2(x))\n",
    "\ta = x - 2**(int(log2(x)))\n",
    "\tk = int(log2(x))\n",
    "\treturn lencoding(l) + binary(a,k)\n",
    "\n",
    "def golomb(b, x):\n",
    "\tq = int((x) / b)\n",
    "\tr = int((x) % b)\n",
    "\tl = int(ceil(log2(b)))\n",
    "\t#print(q,r,l)\n",
    "\treturn unary(q) + binary(r, l)\n",
    "\n",
    "def elias_gamma(x):\n",
    "    return elias_generic(unary, x)\n",
    "\n",
    "def elias_delta(x):\n",
    "    return elias_generic(elias_gamma,x)\n",
    "\n",
    "print(\"%-46s %-20s\" %\n",
    "      (\" \",\"Elias\"))\n",
    "print(\"%5s: %-11s : %-10s : %-10s : %-10s : %-10s\" %\n",
    "      (\"Num\", \"Unary\", \"Binary\", \"Gamma\", \"Delta\", \"Goloumb\"))\n",
    "for i in range(11):\n",
    "\tprint(\"%5d: %-11s : %-10s : %-10s : %-10s : %-10s\" %\n",
    "\t(i,unary(i),binary(i),elias_gamma(i),elias_delta(i), golomb(3,i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing tf, tf-idf\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02192\n",
      "\tWord: 2, TF-IDF: 0.02192\n",
      "\tWord: from, TF-IDF: 0.01096\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n"
     ]
    }
   ],
   "source": [
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = tb(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")\n",
    "\n",
    "bloblist = [document1, document2, document3]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative method to compute TF-IDF\n",
    "\n",
    "#Defining the TF function\n",
    "def computeTF(wordDict,bow):\n",
    "        tfDict = {}\n",
    "        bowCount = len(bow)\n",
    "        for word, count in wordDict.items():\n",
    "                tfDict[word] = count / float(bowCount)\n",
    "        return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bed  cat  dog  face  my  on  sat  the\n",
      "0    0    1    0     1   1   1    1    1\n",
      "1    1    0    1     0   1   1    1    1\n"
     ]
    }
   ],
   "source": [
    "#Calculating TF for two docs\n",
    "\n",
    "docA = \"the cat sat on my face\"\n",
    "docB = \"the dog sat on my bed\"\n",
    "\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "\n",
    "wordSet= set(bowA).union(set(bowB)) #finding vocabulary list\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "for word in bowA:\n",
    "        wordDictA[word]+=1\n",
    "\n",
    "for word in bowB:\n",
    "        wordDictB[word]+=1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bag = pd.DataFrame([wordDictA, wordDictB])\n",
    "\n",
    "#printing wordbag\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Matrix for doc A: \n",
      "{'my': 0.16666666666666666, 'bed': 0.0, 'face': 0.16666666666666666, 'dog': 0.0, 'the': 0.16666666666666666, 'cat': 0.16666666666666666, 'on': 0.16666666666666666, 'sat': 0.16666666666666666}\n",
      "Term Frequency Matrix for doc B: \n",
      "{'my': 0.16666666666666666, 'bed': 0.16666666666666666, 'face': 0.0, 'dog': 0.16666666666666666, 'the': 0.16666666666666666, 'cat': 0.0, 'on': 0.16666666666666666, 'sat': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "#computing term frequency\n",
    "\n",
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "print(\"Term Frequency Matrix for doc A: \")\n",
    "print(tfBowA)\n",
    "\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "print(\"Term Frequency Matrix for doc B: \")\n",
    "print(tfBowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining IDF\n",
    "def computeIDF(docList):\n",
    "        idfDict = {}\n",
    "        N = len(docList)\n",
    "        #Count N of docs that contain word w\n",
    "        idfDict = dict.fromkeys(docList[0].keys(),0)\n",
    "        for doc in docList:\n",
    "                for word, val in doc.items():\n",
    "                        if val > 0:\n",
    "                                idfDict[word] +=1\n",
    "        for word, val in idfDict.items():\n",
    "                idfDict[word] = math.log(N/ float(val))\n",
    "        return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 0.0,\n",
       " 'bed': 0.6931471805599453,\n",
       " 'face': 0.6931471805599453,\n",
       " 'dog': 0.6931471805599453,\n",
       " 'the': 0.0,\n",
       " 'cat': 0.6931471805599453,\n",
       " 'on': 0.0,\n",
       " 'sat': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inverse document frequency for A and B\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow,idfs):\n",
    "        tfidf = {}\n",
    "        for word, val in tfBow.items():\n",
    "                tfidf[word] = val * idfs[word]\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        bed       cat       dog      face   my   on  sat  the\n",
      "0  0.000000  0.115525  0.000000  0.115525  0.0  0.0  0.0  0.0\n",
      "1  0.115525  0.000000  0.115525  0.000000  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "\n",
    "TF = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
    "\n",
    "print(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.35464863330313684\n",
      "  (0, 1)\t0.49844627974580596\n",
      "  (0, 6)\t0.35464863330313684\n",
      "  (0, 5)\t0.35464863330313684\n",
      "  (0, 4)\t0.35464863330313684\n",
      "  (0, 3)\t0.49844627974580596\n",
      "  (1, 7)\t0.35464863330313684\n",
      "  (1, 6)\t0.35464863330313684\n",
      "  (1, 5)\t0.35464863330313684\n",
      "  (1, 4)\t0.35464863330313684\n",
      "  (1, 2)\t0.49844627974580596\n",
      "  (1, 0)\t0.49844627974580596\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn in python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "response = vectorizer.fit_transform([docA,docB])\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
