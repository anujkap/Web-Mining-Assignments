{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "## Question 1\n",
    "For the following paragraph tokenize it and remove stop words. Prepare a table\n",
    "that includes “Word” and “frequency (2 columns).Print the frequency of all words. How\n",
    "many words are there with starting letter A,S,E? Print the maximum frequency word.\n",
    "Segregate all vowels and print them separately (You can use NLTK tool only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "##entering paragraph\n",
    "sent = '''The school fair is right around the corner, and tickets have just gone on sale. We are selling a\n",
    "limited number of tickets at a discount, so move fast and get yours while they are still available.\n",
    "This is going to be an event you will not want to miss! First off, the school fair is a great value\n",
    "when compared with other forms of entertainment. Also, your ticket purchase will help our\n",
    "school, and when you help the school, it helps the entire community. But that’s not all! Every\n",
    "ticket you purchase enters you in a drawing to win fabulous prizes. And don’t forget, you will\n",
    "have mountains of fun because there are acres and acres of great rides, fun games, and\n",
    "entertaining attractions! Spend time with your family and friends at our school fair. Buy your\n",
    "tickets now!'''\n",
    "        \n",
    "\n",
    "##word tokenize\n",
    "word = nltk.word_tokenize(sent)\n",
    "\n",
    "##count vowel frequency\n",
    "word_freq = {}\n",
    "for tok in word:\n",
    "    if tok in word_freq:\n",
    "        word_freq[tok] += 1\n",
    "    elif tok[0] in 'AEIOUaeiou':\n",
    "        word_freq[tok] = 1\n",
    "\n",
    "##counting word frequency\n",
    "word_freq2 = {}\n",
    "for tok in word:\n",
    "    if tok in word_freq2:\n",
    "        word_freq2[tok] += 1\n",
    "    else:\n",
    "        word_freq2[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exporting frequency table into excel\n",
    "df = pd.DataFrame(list(word_freq2.items()),columns=['Word','Frequency'])\n",
    "df.to_excel('test.xlsx', sheet_name='sheet1', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting distribution of tokens\n",
    "fd = nltk.FreqDist(word)\n",
    "fd.plot(50,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mWord        Frequency\u001b[0m\n",
      "The             1    \n",
      "school          5    \n",
      "fair            3    \n",
      "is              3    \n",
      "right           1    \n",
      "around          1    \n",
      "the             4    \n",
      "corner          1    \n",
      ",               9    \n",
      "and             6    \n",
      "tickets         3    \n",
      "have            2    \n",
      "just            1    \n",
      "gone            1    \n",
      "on              1    \n",
      "sale            1    \n",
      ".               6    \n",
      "We              1    \n",
      "are             3    \n",
      "selling         1    \n",
      "a               4    \n",
      "limited         1    \n",
      "number          1    \n",
      "of              4    \n",
      "at              2    \n",
      "discount        1    \n",
      "so              1    \n",
      "move            1    \n",
      "fast            1    \n",
      "get             1    \n",
      "yours           1    \n",
      "while           1    \n",
      "they            1    \n",
      "still           1    \n",
      "available       1    \n",
      "This            1    \n",
      "going           1    \n",
      "to              3    \n",
      "be              1    \n",
      "an              1    \n",
      "event           1    \n",
      "you             5    \n",
      "will            3    \n",
      "not             2    \n",
      "want            1    \n",
      "miss            1    \n",
      "!               4    \n",
      "First           1    \n",
      "off             1    \n",
      "great           2    \n",
      "value           1    \n",
      "when            2    \n",
      "compared        1    \n",
      "with            2    \n",
      "other           1    \n",
      "forms           1    \n",
      "entertainment   1    \n",
      "Also            1    \n",
      "your            3    \n",
      "ticket          2    \n",
      "purchase        2    \n",
      "help            2    \n",
      "our             2    \n",
      "it              1    \n",
      "helps           1    \n",
      "entire          1    \n",
      "community       1    \n",
      "But             1    \n",
      "that            1    \n",
      "’               2    \n",
      "s               1    \n",
      "all             1    \n",
      "Every           1    \n",
      "enters          1    \n",
      "in              1    \n",
      "drawing         1    \n",
      "win             1    \n",
      "fabulous        1    \n",
      "prizes          1    \n",
      "And             1    \n",
      "don             1    \n",
      "t               1    \n",
      "forget          1    \n",
      "mountains       1    \n",
      "fun             2    \n",
      "because         1    \n",
      "there           1    \n",
      "acres           2    \n",
      "rides           1    \n",
      "games           1    \n",
      "entertaining    1    \n",
      "attractions     1    \n",
      "Spend           1    \n",
      "time            1    \n",
      "family          1    \n",
      "friends         1    \n",
      "Buy             1    \n",
      "now             1    \n"
     ]
    }
   ],
   "source": [
    "##class to print headers in color\n",
    "class color:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "print(\"\\n{:<15} {:<5}\".format(color.BOLD + 'Word','Frequency' + color.END))\n",
    "for key in word_freq2:\n",
    "    print(\"{:<15} {:<5}\".format(key,word_freq2[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_with = {}\n",
    "\n",
    "for key in word_freq2:\n",
    "    if key[0] in 'ASE':\n",
    "        starts_with[key] = word_freq2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mWord        Frequency\u001b[0m\n",
      "Also            1    \n",
      "Every           1    \n",
      "And             1    \n",
      "Spend           1    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<5}\".format(color.BOLD + 'Word','Frequency' + color.END))\n",
    "for key in starts_with:\n",
    "    print(\"{:<15} {:<5}\".format(key,starts_with[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mVowel       Frequency\u001b[0m\n",
      "is              3    \n",
      "around          1    \n",
      "and             6    \n",
      "on              1    \n",
      "are             3    \n",
      "a               4    \n",
      "of              4    \n",
      "at              2    \n",
      "available       1    \n",
      "an              1    \n",
      "event           1    \n",
      "off             1    \n",
      "other           1    \n",
      "entertainment   1    \n",
      "Also            1    \n",
      "our             2    \n",
      "it              1    \n",
      "entire          1    \n",
      "all             1    \n",
      "Every           1    \n",
      "enters          1    \n",
      "in              1    \n",
      "And             1    \n",
      "acres           2    \n",
      "entertaining    1    \n",
      "attractions     1    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n{:<15} {:<5}\".format(color.BOLD + 'Vowel','Frequency' + color.END))\n",
    "for key in word_freq:\n",
    "    print(\"{:<15} {:<5}\".format(key,word_freq[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most frequent words are: \n",
      "and : 6\n",
      "school : 5\n",
      "you : 5\n",
      "the : 4\n",
      "a : 4\n"
     ]
    }
   ],
   "source": [
    "max_dict = {}\n",
    "while len(max_dict) < 5:\n",
    "    max_val = 0\n",
    "    for key in word_freq2:\n",
    "        if max_val < word_freq2[key] and re.match(r'[A-Za-z]+',key) and key not in max_dict:\n",
    "            max_key = key\n",
    "            max_val = word_freq2[key]\n",
    "    max_dict[max_key] = max_val\n",
    "    \n",
    "print(\"The five most frequent words are: \")\n",
    "for key in max_dict:\n",
    "    print(key,\":\",max_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobjohn/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted spam - 19 predicted nspam - 1823\n",
      "actual spam - 726 actual nspam - 1116\n",
      "[[1113  710]\n",
      " [   3   16]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plotter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pdb\n",
    "\n",
    "\n",
    "# roughly 60/40 spam not spam split will have the following stats\n",
    "# spam = 1813, 725\n",
    "# not spam = 2788, 1672\n",
    "\n",
    "# printing the entire data frame\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "#### Confusion Matrix helper functions - sklearn metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    accuracy = numpy.trace(cm) / float(numpy.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plotter.get_cmap('Blues')\n",
    "\n",
    "    plotter.figure(figsize=(10, 10))\n",
    "    plotter.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plotter.title(title)\n",
    "    plotter.colorbar()\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plotter.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plotter.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plotter.tight_layout()\n",
    "    plotter.ylabel('True label')\n",
    "    plotter.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(\n",
    "        accuracy, misclass))\n",
    "    plotter.show()\n",
    "\n",
    "\n",
    "class spambase(object):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# OOP style\n",
    "\t\tself.data = pd.read_csv('/Users/jacobjohn/Codes/Web-Mining-Assignments/Final_Exam/spambase/spambase.data', header=None)\n",
    "\t\tself.data.rename(columns={57: 'is_spam'}, inplace=True)\n",
    "\t\tself.statisticsSpam = dict()\n",
    "\t\tself.statisticsNotSpam = dict()\n",
    "\n",
    "\t# split train test data\n",
    "\tdef generateData(self):\n",
    "\t\t# separate dataset into spam and not spam\n",
    "\t\tspam = self.data[self.data['is_spam'] == 1]\n",
    "\t\tnotSpam = self.data[self.data['is_spam'] == 0]\n",
    "\t\t# split using sklearn test train split - random 0.6 for training and 0.4 for testing\n",
    "\t\tspam_train, spam_test = train_test_split(spam, train_size=0.6)\n",
    "\t\tnotSpam_train, notSpam_test = train_test_split(notSpam, train_size=0.6)\n",
    "\t\t# divide the data from labels for easier fitting\n",
    "\t\ttrainData = notSpam_train.append(spam_train)\n",
    "\t\ttrainLabels = trainData.pop('is_spam')\n",
    "\t\ttestData = notSpam_test.append(spam_test)\n",
    "\t\ttestLabels = testData.pop('is_spam')\n",
    "\t\t# return them\n",
    "\t\treturn trainData, trainLabels, testData.values.tolist(), testLabels.values.tolist()\n",
    "\n",
    "\tdef generateStatistics(self, train, trainLabels):\n",
    "    \n",
    "\t\t# converting to list for better manipulation\n",
    "\t\ttrain = train.values.tolist()\n",
    "\t\ttrainLabels = trainLabels.values.tolist()\n",
    "\t\tspam, notSpam = [], []\n",
    "\t\tfor index, (row, label) in enumerate(zip(train, trainLabels)):\n",
    "\t\t\tif label == 1:\n",
    "\t\t\t\tspam.append(train[index])\n",
    "\t\t\telse:\n",
    "\t\t\t\tnotSpam.append(train[index])\n",
    "\n",
    "\t\t# converting to pandas for better statistic calculation\n",
    "\t\tspamdf = pd.DataFrame(spam)\n",
    "\t\tnotSpamdf = pd.DataFrame(notSpam)\n",
    "\n",
    "\t\t# find mean, std for each feature given its class\n",
    "\t\tfor column in spamdf.columns:\n",
    "\t\t\tstrColumn = str(column)\n",
    "\t\t\tmeanOfCol = spamdf[column].mean()\n",
    "\t\t\tstdOfCol = spamdf[column].std()\n",
    "\t\t\t# to avoid divide by zero error in gaussian naive bayes\n",
    "\t\t\tif meanOfCol == 0:\n",
    "\t\t\t\tmeanOfCol = 0.00001\n",
    "\t\t\tif stdOfCol == 0:\n",
    "\t\t\t\tstdOfCol = 0.000001\n",
    "\t\t\tself.statisticsSpam[strColumn] = [meanOfCol, stdOfCol]\n",
    "\n",
    "\t\t# find mean, std for each feature given its class\n",
    "\t\tfor column in notSpamdf.columns:\n",
    "\t\t\tstrColumn = str(column)\n",
    "\t\t\tmeanOfCol = notSpamdf[column].mean()\n",
    "\t\t\tstdOfCol = notSpamdf[column].std()\n",
    "\t\t\t# to avoid divide by zero error in gaussian naive bayes\n",
    "\t\t\tif meanOfCol == 0:\n",
    "\t\t\t\tmeanOfCol = 0.00001\n",
    "\t\t\tif stdOfCol == 0:\n",
    "\t\t\t\tstdOfCol = 0.000001\n",
    "\t\t\tself.statisticsNotSpam[strColumn] = [meanOfCol, stdOfCol]\n",
    "\t\t# pdb.set_trace()\n",
    "\t\treturn self.statisticsNotSpam, self.statisticsSpam\n",
    "\n",
    "\tdef alternativeStatistics(self, train, trainLabels):\n",
    "\t\tspamStats, notSpamStats = {}, {}\n",
    "\t\ttrain = train.values.tolist()\n",
    "\t\ttrainLabels = trainLabels.values.tolist()\n",
    "\n",
    "\t\tspam, notspam = list(), list()\n",
    "\n",
    "\t\tfor data, label in zip(train, trainLabels):\n",
    "\t\t\tif label == 0:\n",
    "\t\t\t\tnotspam.append(data)\n",
    "\t\t\telse:\n",
    "\t\t\t\tspam.append(data)\n",
    "\n",
    "\t\tnumpySpam = numpy.array(spam).T\n",
    "\t\tnumpyNSpam = numpy.array(notspam).T\n",
    "\t\tfor idx, row in enumerate(numpySpam):\n",
    "\t\t\tspamStats[str(idx)] = [numpy.mean(row), numpy.std(row)]\n",
    "\t\t# print(numpySpam.shape, numpyNSpam.shape)\n",
    "\t\tfor idx, row in enumerate(numpyNSpam):\n",
    "\t\t\tnotSpamStats[str(idx)] = [numpy.mean(row), numpy.std(row)]\n",
    "\t\tpdb.set_trace()\n",
    "\n",
    "\t# computes prior values for the two classes - count spam/not spam instances\n",
    "\tdef computePrior(self, labels):\n",
    "\t\t# priorSpam, priorNSpam = 0.0, 0.0\n",
    "\t\tpriorSpam = (labels == 1).astype(int).sum() / len(labels)\n",
    "\t\tpriorNSpam = (labels == 0).astype(int).sum() / len(labels)\n",
    "\t\t# 0.606, 0.393 approx. if you print them out.\n",
    "\t\treturn priorSpam, priorNSpam\n",
    "\n",
    "\tdef gaussianProbability(self, feature, mean, std):\n",
    "\t\t# computes gaussian probability - given in the slides\n",
    "\t\tfirst_term = (1 / numpy.sqrt(2 * numpy.pi * numpy.power(std, 2)))\n",
    "\t\tsecond_term = numpy.exp(-numpy.power((feature - mean),\n",
    "                                       2) / 2 * numpy.power(std, 2))\n",
    "\t\tprob = first_term * second_term\n",
    "\t\tif prob == 0:\n",
    "\t\t\tprob = 0.0001\n",
    "\t\treturn numpy.log(prob)\n",
    "\n",
    "\t# predicting test instances using gaussian probability\n",
    "\tdef predict(self, testInstance, testLabel, priors):\n",
    "\t\t# appending the prior terms first.\n",
    "\t\tpositiveProbabilities = list()\n",
    "\t\tpositiveProbabilities.append(priors[0])\n",
    "\t\tnegativeProbabilites = list()\n",
    "\t\tnegativeProbabilites.append(priors[1])\n",
    "\n",
    "\t\tfor i, X in enumerate(testInstance):\n",
    "\t\t\t# converting to string because dict keys are str while enumerate returns an int\n",
    "\t\t\ti = str(i)\n",
    "\t\t\t# calc positive class probability using the dict built above\n",
    "\t\t\tmean, std = self.statisticsSpam[i]\n",
    "\t\t\tposp = self.gaussianProbability(X, mean, std)\n",
    "\t\t\tif numpy.isinf(posp):\n",
    "\t\t\t\tposp = 0.000001\n",
    "\t\t\tpositiveProbabilities.append(posp)\n",
    "\n",
    "\t\t\t# calc negative class probability using the dict built above\n",
    "\t\t\tmean, std = self.statisticsNotSpam[i]\n",
    "\t\t\tnegp = self.gaussianProbability(X, mean, std)\n",
    "\t\t\tif numpy.isinf(negp):\n",
    "\t\t\t\tnegp = 0.000001\n",
    "\t\t\tnegativeProbabilites.append(negp)\n",
    "\t\tpositivePred = sum(positiveProbabilities)\n",
    "\t\tnegativePred = sum(negativeProbabilites)\n",
    "\t\tif positivePred > negativePred:\n",
    "\t\t\treturn 0\n",
    "\t\telse:\n",
    "\t\t\treturn 1\n",
    "\n",
    "\t# need to write accuracy function explicitly. Although its covered in the confusion matrix method.\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\tclassifier = spambase()\n",
    "\ttrain, trainLabels, test, testLabels = classifier.generateData()\n",
    "\tstats = classifier.generateStatistics(train, trainLabels)\n",
    "\t# stats2 = classifier.alternativeStatistics(train, trainLabels)\n",
    "\t# hard coded the priors. But if you actually call the function computePrior() it will give the same thing\n",
    "\t# priors = classifier.computePrior(trainLabels.values.tolist())\n",
    "\tpriors = (0.606, 0.39)\n",
    "\n",
    "\tpredictions = [classifier.predict(instance, label, priors)\n",
    "                for instance, label in zip(test, testLabels)]\n",
    "\n",
    "\tprint(\"predicted spam - {} predicted nspam - {}\".format(predictions.count(1), predictions.count(0)))\n",
    "\tprint(\"actual spam - {} actual nspam - {}\".format(testLabels.count(1), testLabels.count(0)))\n",
    "\t# prints the accuracy, misclassified points, confusion matrix of the two classes.\n",
    "\tcm = confusion_matrix(predictions, testLabels)\n",
    "\tplot_confusion_matrix(confusion_matrix(predictions, testLabels))\n",
    "\tprint(cm)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.9910394265232975\n",
      "Recall is 0.9753086419753086\n"
     ]
    }
   ],
   "source": [
    "## Using previous matrix for precision and recall\n",
    "precision = (1106)/(1106+10)\n",
    "recall = (1106)/(1106+28)\n",
    "\n",
    "print(\"Precision is\", precision)\n",
    "print(\"Recall is\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(Train=False):\n",
    "    data = []\n",
    "\n",
    "    # Read the training data\n",
    "    f = open('/Users/jacobjohn/Codes/Web-Mining-Assignments/Final_Exam/spambase/spambase.data')\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "\n",
    "    X = np.array([x[:-1] for x in data]).astype(np.float)\n",
    "    y = np.array([x[-1] for x in data]).astype(np.float)\n",
    "    del data # free up the memory\n",
    "\n",
    "    if Train:\n",
    "        # returns X_train, X_test, y_train, y_test\n",
    "        return train_test_split(X, y, test_size=0.3, random_state=RandomState())\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "# Base class inspired by https://github.com/scikit-learn/scikit-learn/blob/a1860144aa2083277ba354b0cc46f9eb4acf0db0/sklearn/naive_bayes.py\n",
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict_log_proba(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        log_prob = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict the label probabilities for the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "\n",
    "        Returns:\n",
    "        probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        return np.exp(self._predict_log_proba(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the labels for the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "\n",
    "        Returns:\n",
    "        probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        return self._classes[np.argmax(self._joint_log_likelihood(X), axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"Accuracy for test data and expected labels\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "\n",
    "        Returns:\n",
    "        accuracy_score -- decimal value (0.0-1.0)\n",
    "        \"\"\"\n",
    "\n",
    "        pred = self.predict(X)\n",
    "\n",
    "        score = 0.0\n",
    "        for i in range(pred.shape[0]):\n",
    "            if (pred[i] == y[i]):\n",
    "                score += 1\n",
    "\n",
    "        return score / pred.shape[0]\n",
    "\n",
    "class GaussianBayes(NaiveBayes):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self.priors = unq_counts / y.shape[0] # K x 1\n",
    "        self.num_classes = len(unq)\n",
    "\n",
    "        mean = []\n",
    "        var = []\n",
    "\n",
    "        for y_i in unq:\n",
    "            X_i = X[y == y_i, :]\n",
    "\n",
    "            mean.append(np.mean(X_i, axis=0))\n",
    "            var.append(np.var(X_i, axis=0))\n",
    "\n",
    "        self.mean = self._weights = np.vstack(mean) # K x N\n",
    "        self.var = np.vstack(var)  # K x N\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        prob = []\n",
    "\n",
    "        epsilon = 1e-9\n",
    "\n",
    "        for k in range(self.num_classes):\n",
    "            mean = self.mean[k, :]\n",
    "            var = self.var[k, :] + epsilon # add epsilon so we never divide by zero\n",
    "            gauss = -0.5 * np.sum(np.log(2.0 * np.pi * var))\n",
    "            gauss -= 0.5 * np.sum(np.square(X - mean) / var, axis=1)\n",
    "            prob.append(np.log(self.priors[k]) + gauss)\n",
    "\n",
    "        prob = np.vstack(prob).T\n",
    "        return prob\n",
    "\n",
    "class MultinomialBayes(NaiveBayes):\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self._log_priors = np.log(unq_counts) - np.log(y.shape[0])\n",
    "\n",
    "        # Alpha will be used for smoothing later.\n",
    "        # If set to zero, we could have numerical instability.\n",
    "        if self.alpha == 0.0:\n",
    "            self.alpha = 1e-16\n",
    "\n",
    "        feature_log_probs = []\n",
    "        for k in range(len(unq)):\n",
    "            # Grab all data for the kth label\n",
    "            subset = X[y == k, :]\n",
    "\n",
    "            # We add alpha for smoothing. This means we don't take the\n",
    "            # log of zero in case a feature is missing (=> P(feature) = 0)\n",
    "            counts = np.sum(subset, axis=0) + self.alpha\n",
    "            count_sum = np.sum(counts) + self.alpha * 2\n",
    "\n",
    "            # Subtract the logs (same as division)\n",
    "            feature_log_probs.append(np.log(counts) - np.log(count_sum.reshape(-1,1)))\n",
    "\n",
    "        self._feature_log_prob = np.vstack(feature_log_probs)\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        # Multinomial Bayes is a simple linear classifier in log-space!\n",
    "        return self._log_priors + X.dot(self._feature_log_prob.T)\n",
    "\n",
    "class BernoulliBayes(NaiveBayes):\n",
    "\n",
    "    def __init__(self, alpha=1.0, binarize=0.5):\n",
    "        self.alpha = alpha\n",
    "        self.binarize = binarize\n",
    "\n",
    "    def __binarize(self, X):\n",
    "        X_bin = np.zeros(X.shape)\n",
    "        X_bin[X > self.binarize] = 1\n",
    "        return X_bin\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the Naive Bayes model to the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "        y --  M x 1 numpy array, storing K unique labels\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        unq, unq_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        self._classes = unq # K x 1\n",
    "        self._priors = unq_counts / y.shape[0] # K x 1\n",
    "\n",
    "        if self.binarize is not None:\n",
    "            X = self.__binarize(X)\n",
    "\n",
    "        # Alpha will be used for smoothing later.\n",
    "        # If set to zero, we could have numerical instability.\n",
    "        if self.alpha == 0.0:\n",
    "            self.alpha = 1e-16\n",
    "\n",
    "        feature_log_probs = []\n",
    "        for k in range(len(unq)):\n",
    "            # Grab all data for the kth label\n",
    "            subset = X[y == k, :]\n",
    "\n",
    "            # We add alpha for smoothing. This means we don't take the\n",
    "            # log of zero in case a feature is missing (=> P(feature) = 0)\n",
    "            counts = np.sum(subset, axis=0) + self.alpha\n",
    "            count_sum = np.sum(counts) + self.alpha * 2\n",
    "\n",
    "            # Subtract the logs (same as division)\n",
    "            feature_log_probs.append(np.log(counts) - np.log(count_sum.reshape(-1,1)))\n",
    "\n",
    "        self._feature_log_prob = np.vstack(feature_log_probs)\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Predict the log of the label probabilities for the input\n",
    "\n",
    "        Arguments:\n",
    "        X -- M x N numpy array\n",
    "\n",
    "        Returns:\n",
    "        log_probabilities -- M x K numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        if self.binarize is not None:\n",
    "            X = self.__binarize(X)\n",
    "\n",
    "        # log of the Bernoulli equation\n",
    "        neg_prob = np.log(1. - np.exp(self._feature_log_prob))\n",
    "        log_priors = np.log(self._priors)\n",
    "        return X.dot((self._feature_log_prob - neg_prob).T) + neg_prob.sum(axis=1) + log_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy. Avg: 0.89287, Std: 0.00658\n",
      "AUC. Avg: 0.95081, Std: 0.00459\n",
      "Top 5 features:\n",
      "[0. 1.]\n",
      "[('credit', -0.6207793480564678), ('people', -1.0103339661143567), ('hpl', -1.0960800006600735), ('font', -1.2289612686544809), ('edu', -1.370197434077716)]\n",
      "[('credit', -0.14593109225361722), ('font', -0.23580322003733123), ('people', -0.5644196402480679), ('3d', -0.7060090942023534), ('over', -0.7272690936292019)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "word_labels = ['address', 'all', '3d', 'our', 'over', 'remove', 'internet',\n",
    "               'order', 'mail', 'receive', 'will', 'people', 'report', 'addresses',\n",
    "               'free', 'business', 'email', 'you', 'credit', 'your', 'font', '000',\n",
    "               'money', 'hp', 'hpl', 'george', '650', 'lab', 'labs', 'telnet', '857',\n",
    "               'data', '415', '85', 'technology', '1999', 'parts', 'pm', 'direct',\n",
    "               'cs', 'meeting', 'original', 'project', 're', 'edu', 'table', 'conference']\n",
    "\n",
    "class METHOD:\n",
    "    gaussian, multinomial, bernoulli = range(3)\n",
    "\n",
    "method = METHOD.bernoulli\n",
    "iterations = 50\n",
    "k = 5\n",
    "binarize = True\n",
    "\n",
    "def find_hyperparams_bernoulli(clf, X, y):\n",
    "    # Set the parameters by cross-validation\n",
    "    param_grid = [{'binarize': [x * 10**-2 for x in range(0, 5000)]}]\n",
    "    grid = GridSearchCV(clf, param_grid)\n",
    "    grid.fit(X, y)\n",
    "    print('done fitting')\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def show_auc(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def top_k_features(k, weights):\n",
    "    return sorted(zip(word_labels, weights), reverse=True, key=operator.itemgetter(1))[:k]\n",
    "\n",
    "def binarize(X, thresh):\n",
    "    X_bin = np.zeros(X.shape)\n",
    "    X_bin[X > thresh] = 1\n",
    "    return X_bin\n",
    "\n",
    "scores = []\n",
    "roc_auc = []\n",
    "weights = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    X_train, X_test, y_train, y_test = load_data(Train=True)\n",
    "\n",
    "    # For now, let's train only on word frequency vectors\n",
    "    X_train = X_train[:, 0:48]\n",
    "    X_test = X_test[:, 0:48]\n",
    "\n",
    "    if method == METHOD.gaussian:\n",
    "        # Gaussian Naive Bayes\n",
    "        # This doesn't really make sense here because our features aren't continuous\n",
    "        # in a way that is Gaussian, they are percentages. This might be better\n",
    "        # for things like number of capital letters.\n",
    "        clf = GaussianNB()\n",
    "\n",
    "    if method == METHOD.multinomial:\n",
    "        # Multinomial Naive Bayes\n",
    "        clf = MultinomialNB(alpha=1.0)\n",
    "\n",
    "    if method == METHOD.bernoulli:\n",
    "        # Bernoulli (multi-variate) Naive Bayes\n",
    "        # It doesn't make sense to include features that are inherently differentiated by magnitude,\n",
    "        # i.e total number of capital letters. So we should only test on word frequencies.\n",
    "        clf = BernoulliNB(alpha=1.0, binarize=0.31) # binarize found via cross validation\n",
    "    #    X, y = load_data()\n",
    "    #    print(find_hyperparams_bernoulli(clf, X[:, 0:48], y))\n",
    "\n",
    "    if binarize:\n",
    "        X_train = binarize(X_train, 0.31)\n",
    "        X_test = binarize(X_test, 0.31)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    roc_auc.append(auc(fpr, tpr))\n",
    "\n",
    "    if method == METHOD.gaussian:\n",
    "        weights = clf.theta_\n",
    "    else:\n",
    "        weights = clf.feature_log_prob_\n",
    "\n",
    "show_auc(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print('Accuracy. Avg: %0.5f, Std: %0.5f' % (np.mean(scores), np.std(scores)))\n",
    "print('AUC. Avg: %0.5f, Std: %0.5f' % (np.mean(roc_auc), np.std(roc_auc)))\n",
    "print('Top %d features:' % k)\n",
    "print(clf.classes_)\n",
    "print(top_k_features(k, weights[0, :]))\n",
    "print(top_k_features(k, weights[1, :]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
