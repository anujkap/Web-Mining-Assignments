{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3#!/usr/ \n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 17 18:10:43 2018\n",
    "\n",
    "@author: jacobjohn\n",
    "\n",
    "A) Consider the following two sentences\n",
    "1. Term frequency matrix is important for ranking docs.\n",
    "2. TFIDF is more important than Term frequency matrix for the same.\n",
    "\n",
    "    i) Find TF MATRIX, IDF values of each term and finally TF*IDF MATRIX.\n",
    "    ii) Find cosine similarity also.\n",
    "\n",
    "\n",
    "B) Implement PAGE RANK ALGORITHM. Take input for adjacency matrix (no need to visualise the directed graph), \n",
    "   find stochastic matrix, find transpose of it. Consider dumping factor 0.7. Consider initial P values as all 1s.  \n",
    "   You can consider 5 nodes. Calculate page rank until 2 iterations and display the ranks.\n",
    "   \n",
    "C) Implement Ellias Gamma, Ellias Delta and Golomb coding\n",
    "\"\"\"\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TFIDF  Term  docs.  for  frequency  important  is  matrix  more  ranking  \\\n",
      "0      0     1      1    1          1          1   1       1     0        1   \n",
      "1      1     1      0    1          1          1   1       1     1        0   \n",
      "\n",
      "   same.  than  the  \n",
      "0      0     0    0  \n",
      "1      1     1    1  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TF-IDF\n",
    "\"\"\"\n",
    "\n",
    "#Defining the TF function\n",
    "def computeTF(wordDict,bow):\n",
    "        tfDict = {}\n",
    "        bowCount = len(bow)\n",
    "        for word, count in wordDict.items():\n",
    "                tfDict[word] = count / float(bowCount)\n",
    "        return tfDict\n",
    "    \n",
    "    \n",
    "#Calculating TF for two docs\n",
    "\n",
    "docA = \"Term frequency matrix is important for ranking docs.\"\n",
    "docB = \"TFIDF is more important than Term frequency matrix for the same.\"\n",
    "\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "\n",
    "wordSet= set(bowA).union(set(bowB)) #finding vocabulary list\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "for word in bowA:\n",
    "        wordDictA[word]+=1\n",
    "\n",
    "for word in bowB:\n",
    "        wordDictB[word]+=1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bag = pd.DataFrame([wordDictA, wordDictB])\n",
    "\n",
    "#printing wordbag\n",
    "print(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Matrix for doc A: \n",
      "{'for': 0.125, 'TFIDF': 0.0, 'ranking': 0.125, 'is': 0.125, 'frequency': 0.125, 'docs.': 0.125, 'same.': 0.0, 'than': 0.0, 'more': 0.0, 'Term': 0.125, 'matrix': 0.125, 'the': 0.0, 'important': 0.125}\n",
      "Term Frequency Matrix for doc B: \n",
      "{'for': 0.09090909090909091, 'TFIDF': 0.09090909090909091, 'ranking': 0.0, 'is': 0.09090909090909091, 'frequency': 0.09090909090909091, 'docs.': 0.0, 'same.': 0.09090909090909091, 'than': 0.09090909090909091, 'more': 0.09090909090909091, 'Term': 0.09090909090909091, 'matrix': 0.09090909090909091, 'the': 0.09090909090909091, 'important': 0.09090909090909091}\n"
     ]
    }
   ],
   "source": [
    "#computing term frequency\n",
    "\n",
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "print(\"Term Frequency Matrix for doc A: \")\n",
    "print(tfBowA)\n",
    "\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "print(\"Term Frequency Matrix for doc B: \")\n",
    "print(tfBowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'for': 0.0,\n",
       " 'TFIDF': 0.6931471805599453,\n",
       " 'ranking': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'frequency': 0.0,\n",
       " 'docs.': 0.6931471805599453,\n",
       " 'same.': 0.6931471805599453,\n",
       " 'than': 0.6931471805599453,\n",
       " 'more': 0.6931471805599453,\n",
       " 'Term': 0.0,\n",
       " 'matrix': 0.0,\n",
       " 'the': 0.6931471805599453,\n",
       " 'important': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining IDF\n",
    "def computeIDF(docList):\n",
    "        idfDict = {}\n",
    "        N = len(docList)\n",
    "        #Count N of docs that contain word w\n",
    "        idfDict = dict.fromkeys(docList[0].keys(),0)\n",
    "        for doc in docList:\n",
    "                for word, val in doc.items():\n",
    "                        if val > 0:\n",
    "                                idfDict[word] +=1\n",
    "        for word, val in idfDict.items():\n",
    "                idfDict[word] = math.log(N/ float(val))\n",
    "        return idfDict\n",
    "\n",
    "#inverse document frequency for A and B\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TFIDF  Term     docs.  for  frequency  important   is  matrix      more  \\\n",
      "0  0.000000   0.0  0.086643  0.0        0.0        0.0  0.0     0.0  0.000000   \n",
      "1  0.063013   0.0  0.000000  0.0        0.0        0.0  0.0     0.0  0.063013   \n",
      "\n",
      "    ranking     same.      than       the  \n",
      "0  0.086643  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.063013  0.063013  0.063013  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow,idfs):\n",
    "        tfidf = {}\n",
    "        for word, val in tfBow.items():\n",
    "                tfidf[word] = val * idfs[word]\n",
    "        return tfidf\n",
    "\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "\n",
    "TF = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
    "\n",
    "print(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44554752 0.31701073 0.31701073 0.31701073 0.31701073 0.31701073\n",
      "  0.         0.44554752 0.         0.31701073 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.25096919 0.25096919 0.25096919 0.25096919 0.25096919\n",
      "  0.35272845 0.         0.35272845 0.25096919 0.35272845 0.35272845\n",
      "  0.35272845]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cosine Similairty\n",
    "\"\"\"\n",
    "\n",
    "#define our set of documents\n",
    "documents = (\n",
    "\"Term frequency matrix is important for ranking docs.\",\n",
    "\"TFIDF is more important than Term frequency matrix for the same.\"\n",
    ")\n",
    "\n",
    "#instantiate the Sklearn TF-IDF Vectorizer and transform our documents into the TF-IDF matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.47735956]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the Cosine Similarity between the first document\n",
    "#with each of the other documents of the set\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1, 0, 0]\n",
      " [0, 0, 0, 0, 0]\n",
      " [1, 1, 0, 0, 1]\n",
      " [0, 0, 0, 0, 1]\n",
      " [0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Page Rank\n",
    "\"\"\"\n",
    "\n",
    "#calculating page rank of a given graph\n",
    "import igraph\n",
    "from numpy import *\n",
    "\n",
    "gd = igraph.Graph(directed=True)\n",
    "gd.add_vertices(5) \n",
    "gd.add_edges([(0,1),(0,2),(2,0),(2,1),(2,4),(3,4),(4,3)]) \n",
    "result = gd.get_adjacency()\n",
    "\n",
    "print(gd.get_adjacency())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic matrix is: \n",
      "[[0, 0.5, 0.5, 0, 0]\n",
      " [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      " [0.3333333333333333, 0.3333333333333333, 0, 0, 0.3333333333333333]\n",
      " [0, 0, 0, 0, 1.0]\n",
      " [0, 0, 0, 1.0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#Stochastic matrix calculation\n",
    "stoc = result\n",
    "sum = [0,0,0,0,0]\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        sum[i] += result[i,j]\n",
    "        \n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if sum[i] == 0:\n",
    "            stoc[i,j] = 1/5\n",
    "        else:\n",
    "            if stoc[i,j] > 0:\n",
    "                stoc[i,j] = stoc[i,j]/sum[i]\n",
    "print(\"Stochastic matrix is: \")\n",
    "print(stoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose is: \n",
      "[[0, 0.2, 0.3333333333333333, 0, 0], [0.5, 0.2, 0.3333333333333333, 0, 0], [0.5, 0.2, 0, 0, 0], [0, 0.2, 0, 0, 1.0], [0, 0.2, 0.3333333333333333, 1.0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#Calculating transpose\n",
    "trans = [[stoc[j][i] for j in range(5)] for i in range(5)]\n",
    "\n",
    "print(\"Transpose is: \")\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page rank of 1 is:  0.2\n",
      "Page rank of 2 is:  0.21276595744728455\n",
      "Page rank of 3 is:  0.22000000000000003\n",
      "Page rank of 4 is:  0.26\n",
      "Page rank of 5 is:  0.2\n",
      "\n",
      "Ranks are as follows: \n",
      "P4 >> P3 >> P2 >> P1 >> P5 >> "
     ]
    }
   ],
   "source": [
    "#Page Rank Calculation\n",
    "d = 0.7\n",
    "n = 5\n",
    "m = 5\n",
    "E = [1] * n\n",
    "rank = [1] * n\n",
    "for i in range(n):\n",
    "    E[i] = [1] * m\n",
    "\n",
    "for it in range(2):\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            rank[j] = ((E[i][j])/n)+((1-d)*trans[i][j])*rank[j]\n",
    "\n",
    "for index in range(len(rank)):\n",
    "    print(\"Page rank of\",index+1,\"is: \",rank[index])\n",
    "\n",
    "sort_rank = [i[0] for i in sorted(enumerate(rank), key=lambda x:x[1], reverse = True)]\n",
    "\n",
    "print(\"\\nRanks are as follows: \")\n",
    "for index in sort_rank:\n",
    "    print(\"P\",index+1,\" >> \",end = \"\",sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Num: Gamma      : Delta      : Goloumb   \n",
      "    0: 0          : 0          : 00        \n",
      "    1: 100        : 1000       : 01        \n",
      "    2: 1100       : 11000      : 010       \n",
      "    3: 1101       : 11001      : 100       \n",
      "    4: 11100      : 11010      : 101       \n",
      "    5: 11101      : 11011      : 1010      \n",
      "    6: 111010     : 110110     : 1100      \n",
      "    7: 111011     : 110111     : 1101      \n",
      "    8: 111100     : 111000     : 11010     \n",
      "    9: 111101     : 111001     : 11100     \n",
      "   10: 1111010    : 1110010    : 11101     \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Implement Ellias Gamma, Ellias Delta and Golomb coding\n",
    "'''\n",
    "\n",
    "from math import log,ceil\n",
    "\n",
    "log2 = lambda x: log(x,2)\n",
    "\n",
    "def binary(x, l = 1):\n",
    "\tfmt = '{0:0%db}'%1\n",
    "\treturn fmt.format(x)\n",
    "\n",
    "def unary(x):\n",
    "\treturn x*'1'+'0'\n",
    "\n",
    "def elias_generic(lencoding, x):\n",
    "\tif x == 0: return '0'\n",
    "\tl = 1+int(log2(x))\n",
    "\ta = x - 2**(int(log2(x)))\n",
    "\tk = int(log2(x))\n",
    "\treturn lencoding(l) + binary(a,k)\n",
    "\n",
    "def golomb(b, x):\n",
    "\tq = int((x) / b)\n",
    "\tr = int((x) % b)\n",
    "\tl = int(ceil(log2(b)))\n",
    "\t#print(q,r,l)\n",
    "\treturn unary(q) + binary(r, l)\n",
    "\n",
    "def elias_gamma(x):\n",
    "    return elias_generic(unary, x)\n",
    "\n",
    "def elias_delta(x):\n",
    "    return elias_generic(elias_gamma,x)\n",
    "\n",
    "print(\"%5s: %-10s : %-10s : %-10s\" %\n",
    "      (\"Num\", \"Gamma\", \"Delta\", \"Goloumb\"))\n",
    "for i in range(11):\n",
    "\tprint(\"%5d: %-10s : %-10s : %-10s\" %\n",
    "\t(i,elias_gamma(i),elias_delta(i), golomb(3,i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
